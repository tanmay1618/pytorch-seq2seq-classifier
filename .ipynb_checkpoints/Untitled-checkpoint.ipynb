{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import unicodedata\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "group1 = [\"Ticketing\"]\n",
    "group2 = [\"Sales\"]\n",
    "group3 = [\"Digital Marketing\",\"Customer service\",\"Marketing\"]\n",
    "group3_class_index = range(len(group3))\n",
    "group3_class_dict = dict(zip(group3, group3_class_index))\n",
    "group4 = [x for x in classes[4:]]\n",
    "group4_class_index = range(len(group4))\n",
    "group4_class_dict = dict(zip(group4, group4_class_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(device):\n",
    "        directory_in_str = \"/home/tanmay/Documents/cv/spotmentor/machine-learning-assessment/data/docs\"\n",
    "        dict_data = []\n",
    "        for file in os.listdir(directory_in_str):\n",
    "                filename = file\n",
    "                if filename.endswith(\".json\"):\n",
    "                        full_path = os.path.join(directory_in_str, filename)\n",
    "                        with open(full_path) as f:\n",
    "                                data = json.load(f)\n",
    "                                temp_data = {}\n",
    "                                temp_data[\"description\"]=data[\"jd_information\"][\"description\"]\n",
    "                                temp_data[\"id\"] = int(data[\"_id\"])\n",
    "                                dict_data.append(temp_data)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(dict_data, orient='columns')\n",
    "        df_dep = pd.read_csv('../data/document_departments.csv')\n",
    "        df_dep.columns=[\"id\",\"department\"]\n",
    "        classes_ = df_dep[\"department\"].unique()\n",
    "        df_dep[\"department_new\"] = df_dep[\"department\"].apply(check_exists,classes=classes_)\n",
    "        full_table= df.merge(df_dep,on='id',how='left')\n",
    "        classes_ = df_dep[\"department_new\"].unique()\n",
    "        te = df_dep[\"department_new\"].value_counts()\n",
    "        print(te)\n",
    "        dc = te.to_dict()\n",
    "        no = full_table.shape[0]\n",
    "        weight_list = []\n",
    "        #for it in classes_:\n",
    "        #    weight_list.append(1/dc[it])\n",
    "        weight_tensor = torch.tensor(weight_list,dtype = torch.float ,device=device)\n",
    "        return full_table, classes_, weight_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_emb(file_name):\n",
    "        script_dir = os.getcwd()\n",
    "        abs_file_path = os.path.join(script_dir, file_name)\n",
    "        print(('Loading word embedding from %s'%file_name))\n",
    "        ret = {}\n",
    "        with open(abs_file_path) as inf:\n",
    "                for idx, line in enumerate(inf):\n",
    "                        if (idx >= 10000):\n",
    "                                break\n",
    "                        info = line.strip().split(' ')\n",
    "                        if info[0].lower() not in ret:\n",
    "                                ret[info[0]] = np.array([float(x) for x in info[1:]])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddedTensorFromSentence(sentence,device,word_emb,N_word):\n",
    "        desc_tokens = sentence.split(\" \")\n",
    "        emb_tokens = []\n",
    "        #print(len(desc_tokens))\n",
    "        for token in desc_tokens:\n",
    "                val = word_emb.get(token, np.zeros(N_word, dtype=np.float32))\n",
    "                emb_tokens.append(torch.tensor(val,dtype = torch.float ,device=device).view(1,1,N_word))\n",
    "        #return emb_tokens\n",
    "        return emb_tokens\n",
    "\n",
    "def check_exists(dep,classes):\n",
    "        if( dep in group1):\n",
    "                return \"Ticketing\"\n",
    "        elif(dep in group2):\n",
    "                return \"Sales\"\n",
    "        elif(dep in group3):\n",
    "                return \"group3\"\n",
    "        else:\n",
    "                return \"group4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, 1)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = input\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        #self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        #import pdb;pdb.set_trace();\n",
    "        #output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(hidden)\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_word=100\n",
    "B_word=6\n",
    "hidden_size = 256\n",
    "max_length = 1000\n",
    "SOS_token = 0\n",
    "CLASS_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embedding from ../glove/glove.6B.100d.txt\n",
      "Ticketing    270\n",
      "Name: department_new, dtype: int64\n",
      "Ticketing    212\n",
      "Name: department_new, dtype: int64\n",
      "Ticketing    58\n",
      "Name: department_new, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "word_emb = load_word_emb('../glove/glove.%dB.%dd.txt'%(B_word,N_word))\n",
    "full_table, classes_, weight_tensor = load_data(device)\n",
    "train_df, test_df = train_test_split(full_table, test_size=0.2, random_state=42)\n",
    "print(train_df.department_new.value_counts())\n",
    "print(test_df.department_new.value_counts())\n",
    "CLASS_size = len(classes_)\n",
    "class_index = range(CLASS_size)\n",
    "class_dict = dict(zip(classes_, class_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=max_length):\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        input_length = len(input_tensor)\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "                encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_output= decoder(decoder_hidden)\n",
    "        #decoder_output, decoder_hidden, decoder_attention = decoder(decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        loss = criterion(decoder_output, torch.max(target_tensor, 1)[1])\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder,data_df, n_iters,class_dict, print_every=1000, plot_every=100, learning_rate=0.05):\n",
    "        start = time.time()\n",
    "        plot_losses = []\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "        criterion = nn.NLLLoss()#weight = weight_tensor)\n",
    "        for iter in range(1, n_iters + 1):\n",
    "                #print(iter)\n",
    "                sentence = train_df.iloc[iter - 1][\"description\"]\n",
    "                sentence = normalizeString(sentence)\n",
    "                input_tensor = embeddedTensorFromSentence(sentence,device,word_emb,N_word)\n",
    "                target_class = data_df.iloc[iter - 1][\"department_new\"]\n",
    "                class_index = []\n",
    "                for i in range(CLASS_size):\n",
    "                        class_index.append(0)\n",
    "                class_index[class_dict[target_class]] = 1\n",
    "                #import pdb; pdb.set_trace();\n",
    "                #print(class_index)\n",
    "                target_tensor = torch.tensor(class_index,dtype = torch.long ,device=device).view(1,CLASS_size)\n",
    "                loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "                if iter % print_every == 0:\n",
    "                        print_loss_avg = print_loss_total / print_every\n",
    "                        print_loss_total = 0\n",
    "                        print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                 iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "                if iter % plot_every == 0:\n",
    "                        plot_loss_avg = plot_loss_total / plot_every\n",
    "                        plot_losses.append(plot_loss_avg)\n",
    "                        plot_loss_total = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, max_length, device):\n",
    "        with torch.no_grad():\n",
    "                input_length = len(input_tensor)\n",
    "                encoder_hidden = encoder.initHidden()\n",
    "\n",
    "                encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "                for ei in range(input_length):\n",
    "                        encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "                        encoder_outputs[ei] += encoder_output[0, 0]\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_output, decoder_hidden = decoder(decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateTest(encoder,decoder,test_df,class_dict):\n",
    "        test_size = test_df.shape[0]\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for iter in range(0, test_size + 1):\n",
    "                sentence = test_df.iloc[iter - 1][\"description\"]\n",
    "                sentence = normalizeString(sentence)\n",
    "                input_tensor = embeddedTensorFromSentence(sentence,device,word_emb,N_word)\n",
    "                target_class = test_df.iloc[iter - 1][\"department_new\"]\n",
    "                class_index = []\n",
    "                target_index = class_dict[target_class]\n",
    "                #print(target_index)\n",
    "                y_true.append(target_index)\n",
    "                output = evaluate(encoder, decoder, input_tensor,max_length,device)\n",
    "                topv, topi = output.topk(1)\n",
    "                y_pred.append(topi.numpy()[0][0])\n",
    "        #import pdb; pdb.set_trace();\n",
    "        test_df[\"pred_department\"] = pd.Series(y_pred)\n",
    "        cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        print(\"Accuarcy\")\n",
    "        print(accuracy_score(y_true, y_pred))\n",
    "        print(cnf_matrix)\n",
    "        return(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/Documents/cv/research_paper/Seq2SQL/venv/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 18s (- 5m 32s) (50 5%) 1.1217\n",
      "0m 30s (- 4m 10s) (100 10%) 10.7655\n",
      "0m 40s (- 3m 29s) (150 16%) 5.1842\n",
      "0m 53s (- 3m 13s) (200 21%) 5.6205\n",
      "1m 6s (- 3m 0s) (250 26%) 15.7907\n",
      "1m 20s (- 2m 49s) (300 32%) 40.9089\n",
      "1m 46s (- 2m 55s) (350 37%) 71.0659\n",
      "2m 8s (- 2m 50s) (400 43%) 32.6992\n",
      "2m 33s (- 2m 43s) (450 48%) 32.6481\n",
      "2m 58s (- 2m 32s) (500 53%) 99.9666\n",
      "3m 17s (- 2m 15s) (550 59%) 57.2222\n",
      "3m 26s (- 1m 53s) (600 64%) 23.3089\n",
      "3m 41s (- 1m 34s) (650 69%) 46.8569\n",
      "3m 54s (- 1m 16s) (700 75%) 117.3137\n",
      "4m 6s (- 0m 58s) (750 80%) 73.3028\n",
      "4m 19s (- 0m 41s) (800 86%) 42.3743\n",
      "4m 30s (- 0m 25s) (850 91%) 144.6995\n",
      "4m 43s (- 0m 9s) (900 96%) 172.3737\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'showPlot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-ed943715c3dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mevaluateTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-9559565d86ef>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, data_df, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mplot_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_loss_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mshowPlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'showPlot' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(N_word, hidden_size).to(device)\n",
    "encoder.apply(init_weights)\n",
    "decoder = DecoderRNN(hidden_size, CLASS_size).to(device)\n",
    "decoder.apply(init_weights)\n",
    "n_iterations = train_df.shape[0]\n",
    "trainIters(encoder, decoder, train_df, n_iterations,class_dict, print_every=50, plot_every=10)\n",
    "print(classes_)\n",
    "pred_df = evaluateTest(encoder,decoder,test_df)\n",
    "ticket_df = pred_df[pred_df[\"pred_department\"]==\"Ticketing\"]\n",
    "sales_df = pred_df[pred_df[\"pred_department\"]==\"Sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_group3 = EncoderRNN(N_word, hidden_size).to(device)\n",
    "encoder_group3.apply(init_weights)\n",
    "decoder_group3 = DecoderRNN(hidden_size, CLASS_size).to(device)\n",
    "decoder_group3.apply(init_weights)\n",
    "group3_train_df = train_df[train_df[\"department_new\"]==\"group3\"]\n",
    "group3_test_df = pred_df[test_df[\"pred_department\"]==\"group3\"]\n",
    "n_iterations = train_df.shape[0]\n",
    "trainIters(encoder, decoder, group3_train_df, \"department\", n_iterations, print_every=50, plot_every=10)\n",
    "group3_pred_df = evaluateTest(encoder,decoder, group3_test_df,\"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_group4 = EncoderRNN(N_word, hidden_size).to(device)\n",
    "encoder_group4.apply(init_weights)\n",
    "decoder_group4 = DecoderRNN(hidden_size, CLASS_size).to(device)\n",
    "decoder_group4.apply(init_weights)\n",
    "group4_train_df = train_df[train_df[\"department_new\"]==\"group4\"]\n",
    "group4_test_df = pred_df[test_df[\"department_new\"]==\"group4\"]\n",
    "n_iterations = train_df.shape[0]\n",
    "trainIters(encoder, decoder, group4_train_df, \"department\",group4_class_dict, n_iterations, print_every=50, plot_every=10)\n",
    "group4_pred_df = evaluateTest(encoder,decoder, group4_test_df,\"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = ticket_df.append(sales_df)\n",
    "final_test = final_test.append(group3_test_df)\n",
    "final_test = final_test.append(group4_test_df)\n",
    "y_true = final_test[\"department\"].tolist()\n",
    "y_pred = final_test[\"pred_department\"].tolist()\n",
    "print(\"Accuarcy\")\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
