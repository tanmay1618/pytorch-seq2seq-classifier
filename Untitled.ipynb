{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import unicodedata\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes_=['Ticketing','Sales','Customer service', 'Digital Marketing', 'Marketing',\n",
    "       'Back office ticketing', 'Finance', \n",
    "       'Operations', 'Engineering Design Construction', 'Presales ',\n",
    "       'Administration', 'Content', 'Logistics', 'Marine Engineering ',\n",
    "       'Maintenance', 'IT', 'Technology', 'Recruitment', 'Data entry',\n",
    "       'Marine Deck ', 'Public Relations ', 'Marine Service Steward ',\n",
    "       'Flight Operations', 'Procurement', 'Analytics',\n",
    "       'Airline Ground Operations', 'QA ', 'Learning and Development ',\n",
    "       'Management Consulting', 'Safety ']\n",
    "group1 = [\"Ticketing\"]\n",
    "group2 = [\"Sales\"]\n",
    "group3 = [\"Digital Marketing\",\"Customer service\",\"Marketing\"]\n",
    "group4 = [x for x in all_classes_[4:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(device):\n",
    "        directory_in_str = \"/home/tanmay/Documents/cv/spotmentor/machine-learning-assessment/data/docs\"\n",
    "        dict_data = []\n",
    "        for file in os.listdir(directory_in_str):\n",
    "                filename = file\n",
    "                if filename.endswith(\".json\"):\n",
    "                        full_path = os.path.join(directory_in_str, filename)\n",
    "                        with open(full_path) as f:\n",
    "                                data = json.load(f)\n",
    "                                temp_data = {}\n",
    "                                temp_data[\"description\"]=data[\"jd_information\"][\"description\"]\n",
    "                                temp_data[\"id\"] = int(data[\"_id\"])\n",
    "                                dict_data.append(temp_data)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(dict_data, orient='columns')\n",
    "        df_dep = pd.read_csv('../data/document_departments.csv')\n",
    "        df_dep.columns=[\"id\",\"department\"]\n",
    "        classes_ = df_dep[\"department\"].unique()\n",
    "        df_dep[\"department_new\"] = df_dep[\"department\"].apply(check_exists,classes=classes_)\n",
    "        full_table= df.merge(df_dep,on='id',how='left')\n",
    "        all_classes_ = df_dep[\"department\"].unique()\n",
    "        classes_ = df_dep[\"department_new\"].unique()\n",
    "        te = df_dep[\"department_new\"].value_counts()\n",
    "        print(te)\n",
    "        dc = te.to_dict()\n",
    "        no = full_table.shape[0]\n",
    "        weight_list = []\n",
    "        #for it in classes_:\n",
    "        #    weight_list.append(1/dc[it])\n",
    "        weight_tensor = torch.tensor(weight_list,dtype = torch.float ,device=device)\n",
    "        return full_table, classes_,all_classes_, weight_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_emb(file_name):\n",
    "        script_dir = os.getcwd()\n",
    "        abs_file_path = os.path.join(script_dir, file_name)\n",
    "        print(('Loading word embedding from %s'%file_name))\n",
    "        ret = {}\n",
    "        with open(abs_file_path) as inf:\n",
    "                for idx, line in enumerate(inf):\n",
    "                        if (idx >= 10000):\n",
    "                                break\n",
    "                        info = line.strip().split(' ')\n",
    "                        if info[0].lower() not in ret:\n",
    "                                ret[info[0]] = np.array([float(x) for x in info[1:]])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddedTensorFromSentence(sentence,device,word_emb,N_word):\n",
    "        desc_tokens = sentence.split(\" \")\n",
    "        emb_tokens = []\n",
    "        #print(len(desc_tokens))\n",
    "        for token in desc_tokens:\n",
    "                val = word_emb.get(token, np.zeros(N_word, dtype=np.float32))\n",
    "                emb_tokens.append(torch.tensor(val,dtype = torch.float ,device=device).view(1,1,N_word))\n",
    "        #return emb_tokens\n",
    "        return emb_tokens\n",
    "\n",
    "def check_exists(dep,classes):\n",
    "        \n",
    "        \n",
    "        if( dep in group1):\n",
    "                return \"Ticketing\"\n",
    "        elif(dep in group2):\n",
    "                return \"Sales\"\n",
    "        elif(dep in group3):\n",
    "                return \"group3\"\n",
    "        else:\n",
    "                return \"group4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, 1)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = input\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        #self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        #self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        #import pdb;pdb.set_trace();\n",
    "        #output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(hidden)\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_word=100\n",
    "B_word=6\n",
    "hidden_size = 256\n",
    "max_length = 1000\n",
    "SOS_token = 0\n",
    "CLASS_size = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embedding from ../glove/glove.6B.100d.txt\n",
      "Ticketing    347\n",
      "group4       275\n",
      "Sales        270\n",
      "group3       270\n",
      "Name: department_new, dtype: int64\n",
      "Ticketing    277\n",
      "group4       220\n",
      "Sales        216\n",
      "group3       216\n",
      "Name: department_new, dtype: int64\n",
      "Ticketing    70\n",
      "group4       55\n",
      "group3       54\n",
      "Sales        54\n",
      "Name: department_new, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "word_emb = load_word_emb('../glove/glove.%dB.%dd.txt'%(B_word,N_word))\n",
    "full_table, classes_,all_classes_, weight_tensor = load_data(device)\n",
    "y = full_table[\"department_new\"]\n",
    "train_df, test_df = train_test_split(full_table, test_size=0.2, random_state=42, stratify=y)\n",
    "print(train_df.department_new.value_counts())\n",
    "print(test_df.department_new.value_counts())\n",
    "CLASS_size = len(classes_)\n",
    "class_index = range(CLASS_size)\n",
    "class_dict = dict(zip(classes_, class_index))\n",
    "\n",
    "group3_class_index = range(len(group3))\n",
    "group3_class_dict = dict(zip(group3, group3_class_index))\n",
    "group4_class_index = range(len(group4))\n",
    "group4_class_dict = dict(zip(group4, group4_class_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=max_length):\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        input_length = len(input_tensor)\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "                encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_output= decoder(decoder_hidden)\n",
    "        #decoder_output, decoder_hidden, decoder_attention = decoder(decoder_hidden, encoder_outputs)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        loss = criterion(decoder_output, torch.max(target_tensor, 1)[1])\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder,data_df,column, n_iters,class_dict, print_every=1000, plot_every=100, learning_rate=0.05):\n",
    "        start = time.time()\n",
    "        plot_losses = []\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "        criterion = nn.NLLLoss()#weight = weight_tensor)\n",
    "        for iter in range(1, n_iters + 1):\n",
    "                #print(iter)\n",
    "                sentence = train_df.iloc[iter - 1][\"description\"]\n",
    "                sentence = normalizeString(sentence)\n",
    "                input_tensor = embeddedTensorFromSentence(sentence,device,word_emb,N_word)\n",
    "                target_class = data_df.iloc[iter - 1][column]\n",
    "                class_index = []\n",
    "                for i in range(CLASS_size):\n",
    "                        class_index.append(0)\n",
    "                class_index[class_dict[target_class]] = 1\n",
    "                #import pdb; pdb.set_trace();\n",
    "                #print(class_index)\n",
    "                target_tensor = torch.tensor(class_index,dtype = torch.long ,device=device).view(1,CLASS_size)\n",
    "                loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "                if iter % print_every == 0:\n",
    "                        print_loss_avg = print_loss_total / print_every\n",
    "                        print_loss_total = 0\n",
    "                        print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                                 iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "                if iter % plot_every == 0:\n",
    "                        plot_loss_avg = plot_loss_total / plot_every\n",
    "                        plot_losses.append(plot_loss_avg)\n",
    "                        plot_loss_total = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, max_length, device):\n",
    "        with torch.no_grad():\n",
    "                input_length = len(input_tensor)\n",
    "                encoder_hidden = encoder.initHidden()\n",
    "                encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "                for ei in range(input_length):\n",
    "                        encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "                        encoder_outputs[ei] += encoder_output[0, 0]\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_output= decoder(decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateTest(encoder,decoder,test_df,class_dict,column):\n",
    "        test_size = test_df.shape[0]\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        test_df[\"pred_department\"] = 0\n",
    "        for iter in range(0, test_size + 1):\n",
    "                sentence = test_df.iloc[iter - 1][\"description\"]\n",
    "                sentence = normalizeString(sentence)\n",
    "                input_tensor = embeddedTensorFromSentence(sentence,device,word_emb,N_word)\n",
    "                target_class = test_df.iloc[iter - 1][column]\n",
    "                class_index = []\n",
    "                target_index = class_dict[target_class]\n",
    "                #print(target_index)\n",
    "                y_true.append(target_index)\n",
    "                output = evaluate(encoder, decoder, input_tensor,max_length,device)\n",
    "                topv, topi = output.topk(1)\n",
    "                y_pred.append(topi.numpy()[0][0])\n",
    "                test_df[\"pred_department\"].iloc[iter-1] = topi.numpy()[0][0]\n",
    "        #import pdb; pdb.set_trace();\n",
    "        cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        print(\"Accuarcy\")\n",
    "        print(accuracy_score(y_true, y_pred))\n",
    "        print(cnf_matrix)\n",
    "        return(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/Documents/cv/research_paper/Seq2SQL/venv/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderRNN(N_word, hidden_size).to(device)\n",
    "encoder.apply(init_weights)\n",
    "decoder = DecoderRNN(hidden_size, CLASS_size).to(device)\n",
    "decoder.apply(init_weights)\n",
    "n_iterations = train_df.shape[0]\n",
    "trainIters(encoder, decoder, train_df,\"department_new\", n_iterations,class_dict, print_every=50, plot_every=10)\n",
    "print(classes_)\n",
    "pred_df = evaluateTest(encoder,decoder,test_df, class_dict,\"department_new\")\n",
    "ticket_df = pred_df[pred_df[\"pred_department\"]==\"Ticketing\"]\n",
    "sales_df = pred_df[pred_df[\"pred_department\"]==\"Sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/Documents/cv/research_paper/Seq2SQL/venv/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-39-f8fcbd941ebb>(3)evaluateTest()\n",
      "-> test_size = test_df.shape[0]\n",
      "(Pdb) n\n",
      "> <ipython-input-39-f8fcbd941ebb>(4)evaluateTest()\n",
      "-> y_true = []\n",
      "(Pdb) n\n",
      "> <ipython-input-39-f8fcbd941ebb>(5)evaluateTest()\n",
      "-> y_pred = []\n",
      "(Pdb) import pdb;pdb.set_trace();\n",
      "(Pdb) test_size\n",
      "0\n",
      "(Pdb) test_df\n",
      "Empty DataFrame\n",
      "Columns: [description, id, department, department_new, pred_department]\n",
      "Index: []\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-59d5ca9a518a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mn_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup3_test_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_group3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_group3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup3_train_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"department\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup3_class_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgroup3_pred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluateTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_group3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_group3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup3_test_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup3_class_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"department\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-f8fcbd941ebb>\u001b[0m in \u001b[0;36mevaluateTest\u001b[0;34m(encoder, decoder, test_df, class_dict, column)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-f8fcbd941ebb>\u001b[0m in \u001b[0;36mevaluateTest\u001b[0;34m(encoder, decoder, test_df, class_dict, column)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"description\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder_group3 = EncoderRNN(N_word, hidden_size).to(device)\n",
    "encoder_group3.apply(init_weights)\n",
    "decoder_group3 = DecoderRNN(hidden_size, CLASS_size).to(device)\n",
    "decoder_group3.apply(init_weights)\n",
    "\n",
    "group3_train_df = train_df[train_df[\"department_new\"]==\"group3\"]\n",
    "group3_test_df = pred_df[test_df[\"pred_department\"]==\"group3\"]\n",
    "\n",
    "n_iterations = group3_test_df.shape[0]\n",
    "trainIters(encoder_group3, decoder_group3, group3_train_df,\"department\", n_iterations,group3_class_dict, print_every=10, plot_every=10)\n",
    "group3_pred_df = evaluateTest(encoder_group3,decoder_group3, group3_test_df, group3_class_dict,\"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/Documents/cv/research_paper/Seq2SQL/venv/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Technology': 12, 'Management Consulting': 24, 'Administration': 6, 'Marine Deck ': 15, 'Safety ': 25, 'Public Relations ': 16, 'Flight Operations': 18, 'Marine Service Steward ': 17, 'Operations': 3, 'Back office ticketing': 1, 'Learning and Development ': 23, 'Analytics': 20, 'IT': 11, 'Finance': 2, 'Marketing': 0, 'Marine Engineering ': 9, 'Engineering Design Construction': 4, 'Presales ': 5, 'QA ': 22, 'Airline Ground Operations': 21, 'Data entry': 14, 'Recruitment': 13, 'Procurement': 19, 'Content': 7, 'Logistics': 8, 'Maintenance': 10}\n",
      "0m 5s (- 0m 0s) (50 90%) 4.2519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/Documents/cv/research_paper/Seq2SQL/venv/lib/python3.5/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-e1b776fbb17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup4_class_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_group4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_group4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup4_train_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"department\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup4_class_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgroup4_pred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluateTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup4_test_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgroup4_class_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"pred_department\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-5d881ca1c3c4>\u001b[0m in \u001b[0;36mevaluateTest\u001b[0;34m(encoder, decoder, test_df, class_dict, column)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mtarget_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mclass_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;31m#print(target_index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "encoder_group4 = EncoderRNN(N_word, hidden_size).to(device)\n",
    "encoder_group4.apply(init_weights)\n",
    "CLASS_size = len(group4)\n",
    "decoder_group4 = DecoderRNN(hidden_size, CLASS_size).to(device)\n",
    "decoder_group4.apply(init_weights)\n",
    "group4_train_df = train_df[train_df[\"department_new\"]==\"group4\"]\n",
    "group4_test_df = pred_df[test_df[\"department_new\"]==\"group4\"]\n",
    "n_iterations = group4_test_df.shape[0]\n",
    "print(group4_class_dict)\n",
    "trainIters(encoder_group4, decoder_group4, group4_train_df,\"department\", n_iterations,group4_class_dict, print_every=50, plot_every=10)\n",
    "group4_pred_df = evaluateTest(encoder,decoder, group4_test_df,group4_class_dict,\"pred_department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer serviceDigital MarketingSalesBack office ticketingTicketingFinanceMarketingOperationsEngineering Design ConstructionPresales AdministrationContentLogisticsMarine Engineering MaintenanceITTechnologyRecruitmentData entryMarine Deck Public Relations Marine Service Steward Flight OperationsProcurementAnalyticsAirline Ground OperationsQA Learning and Development Management ConsultingSafety \n"
     ]
    }
   ],
   "source": [
    "print(all_classes_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = ticket_df.append(sales_df)\n",
    "final_test = final_test.append(group3_test_df)\n",
    "final_test = final_test.append(group4_test_df)\n",
    "y_true = final_test[\"department\"].tolist()\n",
    "y_pred = final_test[\"pred_department\"].tolist()\n",
    "print(\"Accuarcy\")\n",
    "print(accuracy_score(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
